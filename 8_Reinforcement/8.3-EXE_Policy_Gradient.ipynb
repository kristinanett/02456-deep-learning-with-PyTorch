{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "8_3_EXE_Policy_Gradient_real.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kristinanett/02456-deep-learning-with-PyTorch/blob/master/8_Reinforcement/8.3-EXE_Policy_Gradient.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWy7prGN2szz"
      },
      "source": [
        "# Solve cartpole with REINFORCE\n",
        "\n",
        "> By Jonas Busk ([jbusk@dtu.dk](mailto:jbusk@dtu.dk))\n",
        "\n",
        "**2019 update:** Changes have been made to the display of environments due to the previous `viewer` being incompatible with newer versions of Gym.\n",
        "\n",
        "In this part, we will create an agent that can learn to solve the [cartpole problem](https://gym.openai.com/envs/CartPole-v0/) from OpenAI Gym by applying a simple policy gradient method called REINFORCE.\n",
        "In the cartpole problem, we need to balance a pole on a cart that moves along a track by applying left and right forces to the cart.\n",
        "\n",
        "We will implement a probabilistic policy, that given a state of the environment, $s$, outputs a probability distribution over available actions, $a$:\n",
        "\n",
        "$$\n",
        "p_\\theta(a|s)\n",
        "$$\n",
        "\n",
        "The policy is a neural network with parameters $\\theta$ that can be trained with gradient descent.\n",
        "When the set of available actions is discrete, we can use a network with softmax output do describe the distribution.\n",
        "The core idea of training the policy network is quite simple: *we want to maximize the expected total reward by increasing the probability of good actions and decreasing the probability of bad actions*. \n",
        "\n",
        "To achieve this, we apply the gradient of the expected discounted total reward (return):\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\nabla_\\theta \\mathbb{E}[R|\\theta] &= \\nabla_\\theta \\int p_\\theta(a|s) R({a}) \\, da \\\\\n",
        "&= \\int \\nabla_\\theta p_\\theta(a|s) R(a)  \\, da \\\\\n",
        "&= \\int p_\\theta(a|s) \\nabla_\\theta \\log p_\\theta(a|s) R(a) \\, da \\\\\n",
        "&= \\mathbb{E}[\\nabla_\\theta \\log p_\\theta(a|s) R(a)]\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "by definition of expectation and using the identity \n",
        "\n",
        "$$\n",
        "\\nabla_\\theta p_\\theta(a|s) = p_\\theta(a|s) \\nabla_\\theta \\log p_\\theta(a|s) \\ .\n",
        "$$\n",
        "\n",
        "The expectation cannot be evaluated analytically, but we have an environment simulator that when supplied with our current policy $p_\\theta(a|s)$ can return a sequence of *actions*, *states* and *rewards*. This allows us to replace the integral with a Monte Carlo average:\n",
        "\n",
        "$$\n",
        "\\nabla_\\theta \\mathbb{E}[R|\\theta] \\approx \\frac{1}{T} \\sum_{t=0}^T \\nabla_\\theta \\log p_\\theta(a_t|s_t) R_t \\ ,\n",
        "$$\n",
        "\n",
        "which is our final gradient estimator, also known as REINFORCE. In the Monte Carlo estimator we run the environment simulator for a predefined number of steps with actions chosen stochastically according to the current stochastic action network $p_\\theta(a|s)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irztTYN82sz3"
      },
      "source": [
        "*Note: For simple reinforcement learning problems (like the one we will address in this exercise) there are simpler methods that work just fine. However, the Policy Gradient method (with some extensions) has been shown to also work well for complex problems with high dimensional inputs and many parameters, where simple methods become inadequate.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMAe5Yg_2sz4"
      },
      "source": [
        "## Policy gradient"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyxFQenM2zIa",
        "outputId": "bd717b75-6d51-4fc9-a3d4-0f251ce63623",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Install colabgymrender to display gym environments in Colab\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!pip install colabgymrender==1.0.2"
      ],
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: colabgymrender==1.0.2 in /usr/local/lib/python3.7/dist-packages (1.0.2)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.7/dist-packages (from colabgymrender==1.0.2) (0.2.3.5)\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.7/dist-packages (from colabgymrender==1.0.2) (2.2)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from colabgymrender==1.0.2) (0.17.3)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from colabgymrender==1.0.2) (4.1.2.30)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym->colabgymrender==1.0.2) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym->colabgymrender==1.0.2) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym->colabgymrender==1.0.2) (1.19.5)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym->colabgymrender==1.0.2) (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->colabgymrender==1.0.2) (0.16.0)\n",
            "Requirement already satisfied: imageio<3.0,>=2.1.2 in /usr/local/lib/python3.7/dist-packages (from moviepy->colabgymrender==1.0.2) (2.4.1)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.7/dist-packages (from moviepy->colabgymrender==1.0.2) (4.62.3)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.7/dist-packages (from moviepy->colabgymrender==1.0.2) (4.4.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from imageio<3.0,>=2.1.2->moviepy->colabgymrender==1.0.2) (7.1.2)\n",
            "Requirement already satisfied: EasyProcess in /usr/local/lib/python3.7/dist-packages (from pyvirtualdisplay->colabgymrender==1.0.2) (0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMWbaH112sz4"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import gym\n",
        "from gym import wrappers\n",
        "from colabgymrender.recorder import Recorder"
      ],
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mz4PCIhG2sz5"
      },
      "source": [
        "First we create the environment:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYHIc0nj2sz6"
      },
      "source": [
        "env = gym.make('CartPole-v0') # Create environment"
      ],
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8Yb5-mS2sz6"
      },
      "source": [
        "A state in this environment is four numbers describing the position of the cart along with the angle and speed of the pole.\n",
        "There are two available actions: push the cart *left* or *right* encoded as 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZWKLzt_2sz7",
        "outputId": "b68ff110-5c03-48b1-c0d4-e011ad0d16e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "s = env.reset()\n",
        "a = env.action_space.sample()\n",
        "print('sample state:', s)\n",
        "print('sample action:', a )"
      ],
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample state: [-0.02525467  0.00505608 -0.04791625 -0.00976101]\n",
            "sample action: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPd8OvGO2sz8"
      },
      "source": [
        "Let us see how the environment looks when we just take random actions. Note that the episode ends when the pole either 1) is more than 15 degrees from vertical, 2) more outside of the frame or 3) the pole is successfully balanced for some fixed duration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWRlBLeU2sz9",
        "outputId": "ca79661e-c030-4cbe-851b-0b8fca2e4e01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 638
        }
      },
      "source": [
        "env = gym.make('CartPole-v0') # Create environment\n",
        "env = Recorder(env, \"./video\") # To display environment in Colab\n",
        "env.reset() # Reset environment\n",
        "\n",
        "# Run environment\n",
        "while True:\n",
        "    env.render() # Render environment\n",
        "    action = env.action_space.sample() # Get a random action\n",
        "    _, _, done, _ = env.step(action) # Take a step\n",
        "    if done: break # Break if environment is done\n",
        "\n",
        "env.close() # Close environment\n",
        "env.play()"
      ],
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13/13 [00:00<00:00, 232.29it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div align=middle><video width='400' height='600'src='data:video/mp4;base64,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' controls>Sorry, seems like your browser doesn't support HTML5 audio/video</video></div>"
            ],
            "text/plain": [
              "<moviepy.video.io.html_tools.HTML2 object>"
            ]
          },
          "metadata": {},
          "execution_count": 201
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqrRget72sz9"
      },
      "source": [
        "Taking random actions does not do a very good job at balancing the pole. Let us now apply the Policy Gradient method described above to solve this task!\n",
        "\n",
        "Let's first define our network and helper functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvTfsDke2sz-"
      },
      "source": [
        "class PolicyNet(nn.Module):\n",
        "    \"\"\"Policy network\"\"\"\n",
        "\n",
        "    def __init__(self, n_inputs, n_hidden, n_outputs, learning_rate):\n",
        "        super(PolicyNet, self).__init__()\n",
        "        # network\n",
        "        self.hidden = nn.Linear(n_inputs, n_hidden)\n",
        "        #self.hidden2 = nn.Linear(n_hidden, 50)\n",
        "        self.out = nn.Linear(n_hidden, n_outputs)\n",
        "        # training\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.hidden(x)\n",
        "        x = F.relu(x)\n",
        "        #x = self.hidden2(x)\n",
        "        #x = F.relu(x)\n",
        "        x = self.out(x)\n",
        "        return F.softmax(x, dim=1)\n",
        "    \n",
        "    def loss(self, action_probabilities, returns):\n",
        "        return -torch.mean(torch.mul(torch.log(action_probabilities), returns))"
      ],
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlTn_jfz2sz-"
      },
      "source": [
        "def compute_returns(rewards, discount_factor):\n",
        "    \"\"\"Compute discounted returns.\"\"\"\n",
        "    returns = np.zeros(len(rewards))\n",
        "    returns[-1] = rewards[-1]\n",
        "    for t in reversed(range(len(rewards)-1)):\n",
        "        returns[t] = rewards[t] + discount_factor * returns[t+1]\n",
        "    return returns"
      ],
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7zKfnfP2sz-"
      },
      "source": [
        "To start with, our policy will be a rather simple neural network with one hidden layer. We can retrieve the shape of the state space (input) and action space (output) from the environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYst7A5k2sz_",
        "outputId": "aff9ac02-0529-4b74-d80c-d8d9980c7dcc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "n_inputs = env.observation_space.shape[0]\n",
        "n_hidden = 20\n",
        "n_outputs = env.action_space.n\n",
        "\n",
        "print('state shape:', n_inputs)\n",
        "print('action shape:', n_outputs)"
      ],
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "state shape: 4\n",
            "action shape: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tw6q53qL2sz_",
        "outputId": "e9570ebb-f384-4a96-de5f-aacf46cc006e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# training settings\n",
        "\n",
        "num_episodes = 900\n",
        "rollout_limit = 500 # max rollout length\n",
        "discount_factor = 1.0 # reward discount factor (gamma), 1.0 = no discount\n",
        "learning_rate = 0.005 #0.001 # you know this by now\n",
        "val_freq = 100 # validation frequency\n",
        "\n",
        "# setup policy network\n",
        "\n",
        "policy = PolicyNet(n_inputs, n_hidden, n_outputs, learning_rate)\n",
        "\n",
        "# train policy network\n",
        "\n",
        "try:\n",
        "    training_rewards, losses = [], []\n",
        "    print('start training')\n",
        "    for i in range(num_episodes):\n",
        "        rollout = []\n",
        "        s = env.reset()\n",
        "        for j in range(rollout_limit):\n",
        "            # generate rollout by iteratively evaluating the current policy on the environment\n",
        "            with torch.no_grad():\n",
        "                a_prob = policy(torch.from_numpy(np.atleast_2d(s)).float())\n",
        "                a = torch.multinomial(a_prob, num_samples=1).squeeze().numpy()\n",
        "            s1, r, done, _ = env.step(a)\n",
        "            rollout.append((s, a, r))\n",
        "            s = s1\n",
        "            if done: break\n",
        "        # prepare batch\n",
        "        rollout = np.array(rollout)\n",
        "        states = np.vstack(rollout[:,0])\n",
        "        actions = np.vstack(rollout[:,1])\n",
        "        rewards = np.array(rollout[:,2], dtype=float)\n",
        "        returns = compute_returns(rewards, discount_factor)\n",
        "        # policy gradient update\n",
        "        policy.optimizer.zero_grad()\n",
        "        a_probs = policy(torch.from_numpy(states).float()).gather(1, torch.from_numpy(actions)).view(-1)\n",
        "        loss = policy.loss(a_probs, torch.from_numpy(returns).float())\n",
        "        loss.backward()\n",
        "        policy.optimizer.step()\n",
        "        # bookkeeping\n",
        "        training_rewards.append(sum(rewards))\n",
        "        losses.append(loss.item())\n",
        "        # print\n",
        "        if (i+1) % val_freq == 0:\n",
        "            # validation\n",
        "            validation_rewards = []\n",
        "            for _ in range(10):\n",
        "                s = env.reset()\n",
        "                reward = 0\n",
        "                for _ in range(rollout_limit):\n",
        "                    with torch.no_grad():\n",
        "                        a_prob = policy(torch.from_numpy(np.atleast_2d(s)).float())\n",
        "                        a = a_prob.argmax().item()\n",
        "                    s, r, done, _ = env.step(a)\n",
        "                    reward += r\n",
        "                    if done: break\n",
        "                validation_rewards.append(reward)\n",
        "            print('{:4d}. mean training reward: {:6.2f}, mean validation reward: {:6.2f}, mean loss: {:7.4f}'.format(i+1, np.mean(training_rewards[-val_freq:]), np.mean(validation_rewards), np.mean(losses[-val_freq:])))\n",
        "    print('done')\n",
        "except KeyboardInterrupt:\n",
        "    print('interrupt')    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start training\n",
            " 100. mean training reward:  28.21, mean validation reward:  29.20, mean loss:  9.6384\n",
            " 200. mean training reward:  44.81, mean validation reward:  63.10, mean loss: 14.4066\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRB76WVf2sz_"
      },
      "source": [
        "# plot results\n",
        "def moving_average(a, n=10) :\n",
        "    ret = np.cumsum(a, dtype=float)\n",
        "    ret[n:] = ret[n:] - ret[:-n]\n",
        "    return ret / n\n",
        "\n",
        "plt.figure(figsize=(16,6))\n",
        "plt.subplot(211)\n",
        "plt.plot(range(1, len(training_rewards)+1), training_rewards, label='training reward')\n",
        "plt.plot(moving_average(training_rewards))\n",
        "plt.xlabel('episode'); plt.ylabel('reward')\n",
        "plt.xlim((0, len(training_rewards)))\n",
        "plt.legend(loc=4); plt.grid()\n",
        "plt.subplot(212)\n",
        "plt.plot(range(1, len(losses)+1), losses, label='loss')\n",
        "plt.plot(moving_average(losses))\n",
        "plt.xlabel('episode'); plt.ylabel('loss')\n",
        "plt.xlim((0, len(losses)))\n",
        "plt.legend(loc=4); plt.grid()\n",
        "plt.tight_layout(); plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzDNgqUt2sz_"
      },
      "source": [
        "Now let's review the solution!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQn788ll2s0A"
      },
      "source": [
        "env = Recorder(env, \"./gym-results\") # wrappers.Monitor(env, \"./gym-results\", force=True) # Create wrapper to display environment\n",
        "s = env.reset()\n",
        "\n",
        "for _ in range(500):\n",
        "    env.render()\n",
        "    a = policy(torch.from_numpy(np.atleast_2d(s)).float()).argmax().item()\n",
        "    s, r, done, _ = env.step(a)\n",
        "    if done: break\n",
        "    \n",
        "# env.close()\n",
        "env.play()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rv6QJUYS2s0A"
      },
      "source": [
        "## Reducing variance\n",
        "\n",
        "By default, this gradient estimator has high variance and therefore variance reduction becomes important to learn more complex tasks.\n",
        "We can reduce variance by subtracting a baseline from the returns, which is unbiased in expectation:\n",
        "\n",
        "$$\n",
        "\\nabla_\\theta \\mathbb{E}[R|\\theta] \\approx \\frac{1}{T} \\sum_{t=0}^T \\nabla_\\theta \\log p_\\theta(a_t|s_t) (R_t-b_t) \\ ,\n",
        "$$\n",
        "\n",
        "where the baseline, $b_t$, is estimated by the return a timestep $t$ averaged over $V$ rollouts.\n",
        "\n",
        "$$\n",
        "b_t = \\frac{1}{V} \\sum_{v=1}^V R_t^{(v)} \\ .\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MI55gTO-2s0A"
      },
      "source": [
        "## Exercises\n",
        "\n",
        "Now it is your turn! Make sure you read and understand the code, then play around with it and try to make it learn better and faster.\n",
        "\n",
        "Experiment with the:\n",
        "\n",
        "* number of episodes\n",
        "* discount factor\n",
        "* learning rate\n",
        "* network layers\n",
        "\n",
        "\n",
        "### Exercise 1 \n",
        "\n",
        "*Describe any changes you made to the code and why you think they improve the agent. Are you able to get solutions consistently?*\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "First I experimented with increasing the number of episodes from 800 to 1500 which improved the overall result. The video lasted for 6 seconds and terminated because the pole was successfully balanced for some fixed duration. The training reward was 180 and validation reward 200 (saturated at 1000 episodes). Since this took a longer time to train and seemed to be overdoing it I settled on 1000 episodes.  \n",
        "\n",
        "Next I started trying different discount factors. The values I tried were: 0.7, 0.9, 0.97 (and the initial 1.0) and I concluded that in this case the higher the value the better the results. With 0.7 and 0.9 the video lasted 0 seconds and the pole fell over instantly, the mean training and validation reward were around 10 and 20 respectively. When increasing the value to 0.97 the result was pretty much the same as with 1.0 resulting in a 6 second video terminated by having successfully balanced for some time. The mean training reward was around 70 and validation around 195. For future experiments I set the value to 1.0 which seemed to be optimal.\n",
        "\n",
        "Then I experimented with increasing the learning rate. I used 0.001 (original), 0.005 and 0.01. Learning rate of 0.005 gave the best results. When increasing the rate further to 0.01 the system became unstable. Due to this change in learning rate I was also able to decrease the number of episodes.\n",
        "\n",
        "Lastly I added one more hidden layer with 50 units which gave a better performance, therefore I added another layer with 100 units which seemed to make the whole thing more unstable so I went back to only having 2 layers in the network.\n",
        "\n",
        "During the experimentation the system did not give good results consistently.\n",
        "\n",
        "### Exercise 2 \n",
        "\n",
        "*Consider the following sequence of rewards produced by an agent interacting with an environment for 10 timesteps:*\n",
        "\n",
        "[0, 1, 1, 1, 0, 1, 1, 0, 0, 0]\n",
        "\n",
        "* *What is the total reward?*\n",
        "* *What is the total future reward in each timestep?*\n",
        "* *What is the discounted future reward in each timestep if $\\gamma = 0.9$?*\n",
        "\n",
        "*Hint: See introdution notebook.*\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "I used the following equations to calculate the total reward, total future reward for each timestep and the discounted future reward in each timestep:\n",
        "\n",
        "\\begin{equation}\n",
        "R = \\sum_{t=0}^{T} r_{t}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "R_{t}=r_{t}+r_{t+1}+r_{t+2}+\\cdots+r_{T}=\\sum_{k=0}^{T-t} r_{t+k}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "R_{t}=r_{t}+\\gamma r_{t+1}+\\gamma^{2} r_{t+2}+\\cdots+\\gamma^{T-t} r_{T}=\\sum_{k=0}^{T-t} \\gamma^{k} r_{t+k}\n",
        "\\end{equation}\n",
        "\n",
        "I obtained the following results (the code for calculating them can be seen at the end of this notebook):\n",
        "\n",
        "* Total reward equals: 5\n",
        "\n",
        "* Total future reward for each timestep is:  [5, 5, 4, 3, 2, 2, 1, 0, 0, 0]\n",
        "\n",
        "* The discounted future reward for each timestep is:  [3.560931, 3.95659, 3.2851, 2.539, 1.71, 1.9, 1.0, 0.0, 0.0, 0.0]\n",
        "\n",
        "### Exercise 3\n",
        "\n",
        "*In the training output, you will sometimes observe the validation reward starts out lower than the training reward but as training progresses they cross over and the validation reward becomes higher than the training reward. How can you explain this behavior?*\n",
        "\n",
        "*Hint: Do we use the policy network in the same way during training and validation?*\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "In the training part, each action is chosen stochasticly based on the different probability at every step: a = torch.multinomial(a_prob, num_samples=1). However, in the validation part, the action which has higher probability is always chosen at every step: a = a_prob.argmax()\n",
        "\n",
        "In the beginning the system is exploring and random actions are better than our network so the validation reward is very low, but as the network is updated, the network is able to take better actions than just random.\n",
        "\n",
        "### Exercise 4\n",
        "\n",
        "*How does the policy gradient method we have used address the exploration-exploitation dilemma?*\n",
        "\n",
        "*Hint: See the introduction notebook about exploration-exploitation.*\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "We use a softmax function in our neural network output therefore we can say that our policy provides a probability distribution over actions and then we can select an action by sampling from this distribution (a = torch.multinomial(a_prob, num_samples=1)). An inherent property of the sampling approach is that it will gradually reduce exploration and start exploiting as it learns and becomes more confident in the most rewarding actions.\n",
        "\n",
        "### Exercise 5 [optional]\n",
        "\n",
        "Extend the code above to reduce variance of the gradient estimator by computing and subtracting the baseline estimate. \n",
        "\n",
        "*Hint: You need to sample a batch of rollouts (now we sample just one) for each update in order to compute the baseline, $b_t$.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJYmyFE8Kcqo"
      },
      "source": [
        "##Exercise 2 code## "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0Jb_s3-DCBX"
      },
      "source": [
        "series = [0, 1, 1, 1, 0, 1, 1, 0, 0, 0]\n",
        "\n",
        "#calculating the total reward \n",
        "print(\"Total reward equals: \" + str(sum(series)))\n",
        "\n",
        "#calculating the total future reward for each timestep\n",
        "total_future_reward = []\n",
        "for i in range(len(series)):\n",
        "  one_step_sum = 0\n",
        "  for j in range(len(series)-i):\n",
        "    one_step_sum += series[i+j]\n",
        "  total_future_reward.append(one_step_sum)\n",
        "\n",
        "print(\"Total future reward for each timestep is: \", total_future_reward)\n",
        "\n",
        "#calculating the discounted future reward for each timestep\n",
        "discount_rate = 0.9\n",
        "discounted_future_reward = []\n",
        "for i in range(len(series)):\n",
        "  one_step_sum = 0\n",
        "  for j in range(len(series)-i):\n",
        "    one_step_sum += (discount_rate**j)*series[i+j]\n",
        "  discounted_future_reward.append(one_step_sum)\n",
        "\n",
        "print(\"The discounted future reward for each timestep is: \", discounted_future_reward)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t23okSLATreH"
      },
      "source": [
        "## Exercise from Michael Nielsen's book ## \n",
        "\n",
        "Exercise from chapter 3: \n",
        "\n",
        "**As discussed above, one way of expanding the MNIST training data is to use small rotations of training images. What's a problem that might occur if we allow arbitrarily large rotations of training images?**\n",
        "\n",
        "Having the numbers rotated by a small angle is not a problem and can be used to help the model generalize better. The problem with bigger rotations is that numbers are not invariant to rotations. When you rotate a 6 by 180 degrees it becomes a 9. When we allow rotations of both 6s and 9s in our training data it will become very difficult to distinguish the two."
      ]
    }
  ]
}